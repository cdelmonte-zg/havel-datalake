{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde88674-cf79-465a-835c-33175dfb35d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing PySpark ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://fonduta.fritz.box:4040\n",
       "SparkContext available as 'sc' (version = 3.3.1, master = local[*], app id = local-1676620636568)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from dataclasses import dataclass\n",
    "from deepdiff import DeepDiff\n",
    "from pprint import pprint\n",
    "from typing import Dict, Any\n",
    "import hashlib\n",
    "from pyspark.sql import SparkSession\n",
    "from elasticsearch import Elasticsearch\n",
    "import pprint\n",
    "from neo4j import GraphDatabase\n",
    "from textwrap import dedent\n",
    "from datetime import datetime\n",
    "from datasketch import MinHash\n",
    "from datasketch import MinHashLSH\n",
    "from datasketch import LeanMinHash\n",
    "from redis import StrictRedis\n",
    "import redis\n",
    "import json\n",
    "import pickle\n",
    "import base64\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from deepdiff import DeepDiff\n",
    "from pprint import pprint\n",
    "from typing import Dict, Any\n",
    "import hashlib\n",
    "from mongoengine import connect, disconnect, Document, ListField, StringField, URLField, DictField\n",
    "import tika\n",
    "from tika import parser\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c623e99a-8762-44cd-a532-21d3d2800bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fonduta.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession object at 0x7fd971530dc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b31be4-f918-4653-9f65-48556f891092",
   "metadata": {},
   "source": [
    "### Packages benÃ¶tigt\n",
    "\n",
    "- datasketch[redis]\n",
    "- elasticsearch==7.10.0\n",
    "- neo4j\n",
    "- pyyaml\n",
    "- deepdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4cfbfb-8949-46f1-a00e-824255a6cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_idx_resource(source, schema, dataset):\n",
    "        return \"%s-%s-%s\"%(source.replace(' ', '_'), schema.replace(' ', '_'), dataset.replace(' ', '_'))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_idx_signature(data):\n",
    "        return Utils.calc_idx_resource(data['source'], data['schema'], data['dataset'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_idx_content(data):\n",
    "        return \"%s-%s-%s-%s\"%(data['source'].replace(' ', '_'), data['schema'].replace(' ', '_'), data['dataset'].replace(' ', '_'), data['attribute'].replace(' ', '_'))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_df_data_types_as_dict(df):\n",
    "        return df.toPandas().dtypes.apply(lambda x: x.name).to_dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def serialize_dict_data_types(data_types):\n",
    "        return json.dumps(data_types, sort_keys=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_dict_data_types(ser_df_data_types):\n",
    "        return  json.loads(ser_df_data_types)\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_dicts(dict_a, dict_b):\n",
    "        return DeepDiff(dict_a, dict_b)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dict_hash(dictionary: Dict[str, Any]) -> str:\n",
    "        \"\"\"MD5 hash of a dictionary.\"\"\"\n",
    "        dhash = hashlib.md5()\n",
    "        encoded = json.dumps(dictionary, sort_keys=True).encode()\n",
    "        dhash.update(encoded)\n",
    "        return dhash.hexdigest()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dbc23-be08-4c2a-9228-9de9fc9ad7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    hadoop_uri = 'hdfs://fonduta.fritz.box:9000'\n",
    "    \n",
    "    search_service_host = 'http://192.168.178.52:9200'\n",
    "    search_service_user = 'admin'\n",
    "    search_service_password = 'password'\n",
    "    \n",
    "    graph_service_host = 'bolt://192.168.178.52:7687'\n",
    "    graph_service_user = 'neo4j'\n",
    "    graph_service_password = 'password'\n",
    "    \n",
    "    data_description_service_host = '192.168.178.52'\n",
    "    data_description_service_port = 27017\n",
    "    data_description_service_host_db_prefix = 'havel'\n",
    "    data_description_service_username='mongoadmin'\n",
    "    data_description_service_password='bdung'\n",
    "    \n",
    "    cache_host = '192.168.178.52'\n",
    "    cache_port = 6379 \n",
    "    cache_db = 4\n",
    "    \n",
    "    content_analyzer_url_base = 'http://master:50070/webhdfs/v1/'\n",
    "    \n",
    "    abs_path = '/sample-data'\n",
    "    \n",
    "    index_content = {\n",
    "        'dev': 'dev-havel-index-content',\n",
    "        'test': 'test-havel-index-content',\n",
    "        'prod': 'prod-havel-index-content'\n",
    "    }\n",
    "\n",
    "    index_signature = {\n",
    "        'dev': 'dev-havel-index-signature',\n",
    "        'test': 'test-havel-index-signature',\n",
    "        'prod': 'prod-havel-index-signature'\n",
    "    }    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ee20a-540b-409f-96dc-4f43b7104e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigLoader:     \n",
    "    @staticmethod\n",
    "    def load_config(file):\n",
    "        config_data = []\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                config = yaml.load(f, Loader=SafeLoader)\n",
    "                for source in config['sources']:\n",
    "                    source_description = config['sources'][source]['description']\n",
    "                    source_name = config['sources'][source]['name']\n",
    "                    version = config['sources'][source]['version']\n",
    "                    url = config['sources'][source]['url']\n",
    "                    datasets = config['sources'][source]['datasets']\n",
    "\n",
    "                    for dataset in datasets:\n",
    "                        dataset_name = dataset['name']\n",
    "                        hdfs_uri_base = \"/\" + Config.abs_path + '/' + source + '/' + source_name\n",
    "\n",
    "                        if ('description' in dataset):\n",
    "                            dataset_description = dataset['description']\n",
    "                        else:\n",
    "                            dataset_description = ''\n",
    "\n",
    "                        if ('keywords_whitelist' in dataset):\n",
    "                            keywords_whitelist = dataset['keywords_whitelist']\n",
    "                        else:\n",
    "                            keywords_whitelist = []\n",
    "\n",
    "                        if ('attributes_blacklist' in dataset):\n",
    "                            attributes_blacklist = dataset['attributes_blacklist']\n",
    "                        else:\n",
    "                            attributes_blacklist = []\n",
    "\n",
    "                        config_data.append({\n",
    "                            'source': source,\n",
    "                            'version': version,\n",
    "                            'source_description': source_description,\n",
    "                            'schema': source_name,\n",
    "                            'dataset': dataset_name,\n",
    "                            'dataset_description': dataset_description,\n",
    "                            'url': url,\n",
    "                            'hdfs_path': '',\n",
    "                            'dataframe': '',\n",
    "                            'keywords_whitelist': keywords_whitelist,\n",
    "                            'attributes_blacklist': attributes_blacklist,\n",
    "                            'hdfs_uri_base': hdfs_uri_base,\n",
    "                            'signature': Utils.calc_idx_resource(source, source_name, dataset_name)\n",
    "                        })\n",
    "\n",
    "                return config_data    \n",
    "\n",
    "        except yaml.YAMLError as e:\n",
    "            print(\"Error in configuration file\", e)\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_to_load_job(list_to_read, config_data):   \n",
    "        data_to_load = []\n",
    "\n",
    "        for i in range(len(list_to_read)):\n",
    "            for j in range(len(config_data)):\n",
    "                if config_data[j]['signature'] == Utils.calc_idx_resource(list_to_read[i]['source'], list_to_read[i]['schema'], list_to_read[i]['dataset']):\n",
    "                    file = config_data[j]['hdfs_uri_base'] + '/' + list_to_read[i]['filename']\n",
    "                    config_data[j]['dataframe'] = spark.read.format(\"csv\")\\\n",
    "                        .option(\"header\", \"true\")\\\n",
    "                        .option(\"inferSchema\", \"true\")\\\n",
    "                        .load(Config.hadoop_uri + file)\n",
    "                    config_data[j]['hdfs_path'] = file\n",
    "                    data_to_load.append(config_data[j])\n",
    "\n",
    "        return data_to_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d63794ca-c1dc-4689-bf0c-f46cc0ac230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchService:\n",
    "    es_res_max_size = 1000\n",
    "    \n",
    "    def __init__(self, uri, user, password, env, index_signature, index_content, debug):\n",
    "        self.driver = Elasticsearch(uri)\n",
    "        self.env = env\n",
    "        self.debug = debug\n",
    "        self.index_signature = index_signature\n",
    "        self.index_content = index_content\n",
    "\n",
    "\n",
    "    def upsert(self, index, func_calc_idx, doc, v=True):\n",
    "        for data in doc:\n",
    "            res = self.driver.index(index=index, id=func_calc_idx(data), body=data)\n",
    "\n",
    "        self.driver.indices.refresh(index=index)\n",
    "\n",
    "        if v is True:\n",
    "            res = self.driver.search(size=SearchService.es_res_max_size, index=index, body={\"query\": {\"match_all\": {}}})\n",
    "            if self.debug: print(\"Got %d Hits.\" % res['hits']['total']['value'])\n",
    "\n",
    "\n",
    "    def clear_es(self):\n",
    "        # delete all\n",
    "        indices = [self.index_content[self.env], self.index_signature[self.env]]\n",
    "        if self.debug: pprint(indices)\n",
    "        try:\n",
    "            self.driver.delete_by_query(index=indices, body={\"query\": {\"match_all\": {}}})\n",
    "            self.driver.indices.delete(index=indices, ignore=[400, 404])\n",
    "        except Exception as e:\n",
    "            if self.debug: print(\"Delete Index: %s\", e)\n",
    "\n",
    "    def reset_all():\n",
    "        all_indices = self.driver.indices.get_alias().keys()\n",
    "        print (\"\\nAttempting to delete\", len(all_indices), \"indexes.\")\n",
    "\n",
    "        for _index in all_indices:\n",
    "            try:\n",
    "                if \".\" not in _index: # avoid deleting indexes like `.kibana`\n",
    "                    self.driver.indices.delete(index=_index)\n",
    "                    if self.debug: print (\"Successfully deleted:\", _index)\n",
    "            except Exception as error:\n",
    "                if self.debug: print ('indices.delete error:', error, 'for index:', _index)\n",
    "\n",
    "    def create_es_schema_signature(self):\n",
    "        schema = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"source\": {\n",
    "                        \"type\": \"text\" # formerly \"string\"\n",
    "                    },\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"text\" # formerly \"string\"\n",
    "                    },\n",
    "                    \"dataset\": {\n",
    "                        \"type\": \"text\" # formerly \"string\"\n",
    "                    },\n",
    "                    \"timestamp\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"yyyy-MM-dd HH:mm:ss.SSSSSS\"\n",
    "                        # data format for Python's datetime.now() method\n",
    "                    },\n",
    "                    \"min_hash\": {\n",
    "                        \"ignore_malformed\": \"true\",\n",
    "                        \"type\": \"integer\"\n",
    "                    },\n",
    "                    \"min_hash_seed\": {\n",
    "                        \"type\": \"integer\"\n",
    "                    },\n",
    "                    \"data_types\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                    \"version\": {\n",
    "                        \"type\": \"integer\"\n",
    "                    },\n",
    "                    \"keywords\": {\n",
    "                        \"type\": \"text\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        resp = self.driver.indices.create(\n",
    "            index = self.index_signature[self.env],\n",
    "            body = schema\n",
    "        )\n",
    "\n",
    "        if self.debug: print (\"_mapping response:\", json.dumps(resp, indent=4), \"\\n\")\n",
    "\n",
    "\n",
    "    def create_es_schema_content(self):\n",
    "        schema = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"source\": {\n",
    "                        \"type\": \"text\" # formerly \"string\"\n",
    "                    },\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                    \"dataset\": {\n",
    "                        \"type\": \"text\" # formerly \"string\"\n",
    "                    },\n",
    "                    \"attribute\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                    \"data_type\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                     \"timestamp\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"yyyy-MM-dd HH:mm:ss.SSSSSS\"\n",
    "                        # data format for Python's datetime.now() method\n",
    "                    },\n",
    "                    \"size\": {\n",
    "                        \"type\": \"integer\"\n",
    "                    },\n",
    "                    \"cardinality\": {\n",
    "                        \"type\": \"float\"\n",
    "                    },\n",
    "                    \"uniqueness\": {\n",
    "                        \"type\": \"float\"\n",
    "                    },\n",
    "                    \"min_hash\": {\n",
    "                        \"ignore_malformed\": \"true\",\n",
    "                        \"type\": \"integer\"\n",
    "                    },\n",
    "                    \"min_hash_seed\": {\n",
    "                        \"type\": \"integer\"\n",
    "                    },\n",
    "                    \"keywords\": {\n",
    "                        \"type\": \"text\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        resp = self.driver.indices.create(\n",
    "            index = self.index_content[self.env],\n",
    "            body = schema\n",
    "        )\n",
    "\n",
    "        if self.debug: print (\"_mapping response:\", json.dumps(resp, indent=4), \"\\n\")\n",
    "\n",
    "\n",
    "    def get_hits(self, index):\n",
    "        res = self.driver.search(size=SearchService.es_res_max_size, index=index, body={\"query\": {\"match_all\": {}}})\n",
    "        if self.debug: print(\"Got %d Hits.\" % res['hits']['total']['value'])\n",
    "        \n",
    "        return (res['hits']['hits'])\n",
    "    \n",
    "    \n",
    "    def search_by_keyword(self, index, keyword):\n",
    "        resp = self.driver.search(\n",
    "            index= index,\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        \"keywords\": keyword\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        res = []\n",
    "        for r in resp['hits']['hits']:\n",
    "            res.append({\n",
    "                'id': r['_id'], \n",
    "                'score': r['_score'],\n",
    "                'source': r['_source']['source'], \n",
    "                'schema': r['_source']['schema'], \n",
    "                'dataset': r['_source']['dataset'], \n",
    "                'attribute': r['_source']['attribute']})\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79811ed-daf3-44fc-ba3f-c81558365e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphService:\n",
    "\n",
    "    def __init__(self, uri, user, password, env, debug):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.env = env\n",
    "        self.debug = debug\n",
    "        \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def insert_column_node(self, idx_col, idx_tab, data):\n",
    "        with self.driver.session() as session:\n",
    "            res1 = session.execute_write(self._create_column_node, idx_col, data, self.env)\n",
    "            res2 = session.execute_write(self._create_column_table_relation, idx_col, idx_tab, self.env)\n",
    "            if self.debug: print(res1)\n",
    "            if self.debug: print(res2)\n",
    "    \n",
    "    def insert_table_node(self, idx, data):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._create_table_node, idx, data, self.env)\n",
    "            if self.debug: print(res)\n",
    "            \n",
    "    def add_column_column_relation(self, idx_a, idx_b, threshold):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._create_column_column_relation, idx_a, idx_b, threshold, self.env)\n",
    "            if self.debug: print(res)\n",
    "    \n",
    "    def add_table_table_relation(self, idx_a, idx_b, threshold):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._create_table_table_relation, idx_a, idx_b, threshold, self.env)\n",
    "            if self.debug: print(res)\n",
    "            \n",
    "    def remove_self_relations(self):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._delete_self_relations, self.env)\n",
    "            if self.debug: print(res)     \n",
    "    \n",
    "    def delete_all(self):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._delete_all, self.env)\n",
    "            if self.debug: print(res)\n",
    "    \n",
    "    \n",
    "    def find_related_tables_by_signature_to_2_grade(self, dataset, weight, dastart, daend):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_read(self._fetch_related_tables_by_signature_to_2_grade, dataset, weight, dastart, daend, self.env)\n",
    "            if self.debug: print(res)\n",
    "\n",
    "            return res\n",
    "        \n",
    "    def find_related_tables_by_content_to_1_grad(self, attribute, weight, dastart, daend):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_read(self._fetch_related_tables_by_content_to_1_grade, attribute, weight, dastart, daend, self.env)\n",
    "            if self.debug: print(res)\n",
    "\n",
    "            return res    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_column_node(tx, idx, data, env):\n",
    "        result = tx.run(\"MERGE (a:Column {idx: $idx, source: $source, schema: $schema, dataset: $dataset, version: $version, env: $env, \"\n",
    "                        \"attribute: $attribute, data_type: $data_type, uniqueness: $uniqueness}) \"\n",
    "                        \"ON CREATE SET a.createdAt = timestamp(), a.updatetAt = timestamp() \"\n",
    "                        \"ON MATCH SET a.updatetAt = timestamp() \"\n",
    "                        \"RETURN a.name + ' created as node ' + id(a) \",\n",
    "                        source=data['source'], schema=data['schema'], dataset=data['dataset'], version=1, timestamp=data['timestamp'],\n",
    "                        attribute=data['attribute'],  data_type=data['data_type'], cardinality=data['cardinality'],  uniqueness=data['uniqueness'], \n",
    "                        env=env, idx=idx)\n",
    "        \n",
    "        return result.single()[0]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_table_node(tx, idx, data, env):\n",
    "        result = tx.run(\"MERGE (a:Table{idx: $idx, source: $source, schema: $schema, dataset: $dataset, version: $version, env: $env}) \"\n",
    "                        \"ON CREATE SET a.createdAt = timestamp(), a.updatetAt = timestamp() \"\n",
    "                        \"ON MATCH SET a.updatetAt = timestamp() \"\n",
    "                        \"RETURN a.name + ' created as node ' + id(a) \", \n",
    "                        source=data['source'], schema=data['schema'], dataset=data['dataset'], version=data['version'], timestamp=data['timestamp'], \n",
    "                        env=env, idx=idx)\n",
    "        return result.single()[0]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_table_table_relation(tx, idx_a, idx_b, weight, env):\n",
    "        result = tx.run(\"MATCH (a:Table {idx: $idx_a, env: $env}), (b:Table {idx: $idx_b, env: $env}) \"\n",
    "                        \"WHERE a <> b \"\n",
    "                        \"MERGE (a)-[r:IS_SIMILAR_TO {weight: $weight}]-(b) \"\n",
    "                        \"ON CREATE SET r.createdAt = timestamp() , r.updatetAt = timestamp()\"\n",
    "                        \"ON MATCH SET r.updatetAt = timestamp() \"\n",
    "                        \"RETURN 'relation ' + type(r) + ' created between ' + a.dataset + ' and ' + b.dataset\",\n",
    "                        idx_a=idx_a, idx_b=idx_b, weight=weight, env=env)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_column_table_relation(tx, idx_col, idx_tab, env):\n",
    "        result = tx.run(\"MATCH (t:Table {idx: $idx_tab, env: $env}) \"\n",
    "                        \"WITH max(t.version) AS maximum \"\n",
    "                        \"MATCH (c:Column {idx: $idx_col, env: $env}), (t:Table {idx: $idx_tab, env: $env}) \"\n",
    "                        \"WHERE t.version = maximum \"\n",
    "                        \"MERGE (c)-[r:IS_ATTRIBUTE_OF {weight: '1.0'}]->(t) \"\n",
    "                        \"ON CREATE SET r.createdAt = timestamp(), r.updatetAt = timestamp() \"\n",
    "                        \"ON MATCH SET r.updatetAt = timestamp() \"\n",
    "                        \"RETURN 'relation ' + type(r) + ' created between ' + c.attribute + ' and ' + t.dataset\", \n",
    "                        idx_tab=idx_tab, idx_col=idx_col, env=env)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_column_column_relation(tx, idx_a, idx_b, weight, env):\n",
    "        result = tx.run(\"MATCH (a:Column {idx: $idx_a, env: $env}), (b:Column {idx: $idx_b, env: $env}) \"\n",
    "                        \"WHERE a.dataset <> b.dataset AND a <> b \"\n",
    "                        \"MERGE (a)-[r:IS_SIMILAR_TO {weight: $weight}]-(b) \"\n",
    "                        \"ON CREATE SET r.createdAt = timestamp(), r.updatetAt = timestamp() \"\n",
    "                        \"ON MATCH SET r.updatetAt = timestamp() \"\n",
    "                        \"RETURN 'relation ' + type(r) + ' created between ' + a.attribute + ' and ' + b.attribute\", \n",
    "                        idx_a=idx_a, idx_b=idx_b, env=env, weight=weight)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _delete_self_relations(tx, env):\n",
    "        result = tx.run(\"MATCH (a {env: $env})-[rel:IS_SIMILAR_TO]->(a) DELETE rel\", env=env)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "   \n",
    "    @staticmethod\n",
    "    def _delete_all(tx, env):\n",
    "        result = tx.run(\"MATCH (n {env: $env}) DETACH DELETE n\", env=env)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fetch_related_tables_by_signature_to_2_grade(tx, dataset, weight, dastart, daend, env):\n",
    "        q = '''\n",
    "            MATCH (a:Table)-[r1:IS_SIMILAR_TO]-(b:Table) \n",
    "            WHERE a.dataset = $dataset AND a.env = $env AND b.env = $env AND r1.weight = $weight\n",
    "                AND $dastart <= r1.updatetAt AND r1.updatetAt <= $daend\n",
    "            OPTIONAL MATCH (b:Table)-[r2:IS_SIMILAR_TO]-(c:Table) \n",
    "            WHERE c.env = $env AND c <> a AND r2.weight = $weight \n",
    "                AND $dastart <= r2.updatetAt AND r2.updatetAt <= $daend\n",
    "            RETURN a, b, c\n",
    "        '''\n",
    "        \n",
    "        result = tx.run(dedent(q), env=env, dataset=dataset, weight=weight, dastart=dastart, daend=daend)\n",
    "\n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append((record[0], record[1], record[2]))\n",
    "\n",
    "        return entire_result\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fetch_related_tables_by_content_to_1_grade(tx, attribute, weight, dastart, daend, env):\n",
    "        q = '''\n",
    "            MATCH (a:Column)-[r1:IS_ATTRIBUTE_OF]-(b:Table)<-[r2:IS_ATTRIBUTE_OF]-(c:Column)-[r3:IS_SIMILAR_TO]-(d:Column)-[r4:IS_ATTRIBUTE_OF]->(e:Table)\n",
    "            WHERE a.attribute = $attribute AND a.env=$env AND b.env=$env AND c.env=$env AND d.env=$env AND e.env=$env AND b <> e AND r3.weight=$weight\n",
    "                AND $dastart <= r1.updatetAt AND r1.updatetAt <= $daend\n",
    "                AND $dastart <= r2.updatetAt AND r2.updatetAt <= $daend\n",
    "                AND $dastart <= r3.updatetAt AND r3.updatetAt <= $daend\n",
    "                AND $dastart <= r4.updatetAt AND r4.updatetAt <= $daend\n",
    "            RETURN b, c, d, e\n",
    "        '''\n",
    "        \n",
    "        result = tx.run(dedent(q), env=env, attribute=attribute, weight=weight, dastart=dastart, daend=daend)\n",
    "\n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append((record[0], record[1], record[2], record[3]))\n",
    "\n",
    "        return entire_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2499dfa0-9766-46e1-bdd1-eb696d9f26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainteinanceService():\n",
    "    \n",
    "    def __init__(self, env, graph_db, search_engine, cache, index_signature, index_content, debug=False):\n",
    "        self.env = env\n",
    "        self.debug = debug\n",
    "        self.graph_db = graph_db\n",
    "        self.search_engine = search_engine\n",
    "        self.cache = cache\n",
    "        self.index_signature = index_signature\n",
    "        self.index_content = index_content\n",
    "        \n",
    "        self.lsh = {\n",
    "            # '0.0':  MinHashLSH(\n",
    "            #     threshold=0.0, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 0}\n",
    "            # }),\n",
    "            # '0.1':  MinHashLSH(\n",
    "            #     threshold=0.1, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 0}\n",
    "            # }),\n",
    "            # '0.2':  MinHashLSH(\n",
    "            #     threshold=0.2, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 0}\n",
    "            # }),\n",
    "            # '0.3':  MinHashLSH(\n",
    "            #     threshold=0.3, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 0}\n",
    "            # }),\n",
    "            # '0.4':  MinHashLSH(\n",
    "            #     threshold=0.4, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 0}\n",
    "            # }),\n",
    "            '0.5':  MinHashLSH(\n",
    "                threshold=0.5, num_perm=128, storage_config={\n",
    "                'type': 'redis',\n",
    "                'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "                'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 0}\n",
    "             }),\n",
    "            # '0.6':  MinHashLSH(\n",
    "            #     threshold=0.6, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 0}\n",
    "            # }),\n",
    "            # '0.7': MinHashLSH(\n",
    "            #     threshold=0.7, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 1}\n",
    "            # }),\n",
    "            '0.8': MinHashLSH(\n",
    "                threshold=0.8, num_perm=128, storage_config={\n",
    "                'type': 'redis',\n",
    "                'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "                'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 2}\n",
    "            }),\n",
    "            '0.9': MinHashLSH(\n",
    "                threshold=0.9, num_perm=128, storage_config={\n",
    "                'type': 'redis',\n",
    "                'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "                'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 2}\n",
    "            }),\n",
    "            '1.0': MinHashLSH(\n",
    "                threshold=1.0, num_perm=128, storage_config={\n",
    "                'type': 'redis',\n",
    "                'basename': bytearray(self.index_content[env], 'utf-8'),\n",
    "                'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 2}\n",
    "             })\n",
    "        }\n",
    "        \n",
    "        self.lsh_sig = {\n",
    "            # '0.0': MinHashLSH(\n",
    "            #     threshold=0.0, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            # '0.1': MinHashLSH(\n",
    "            #     threshold=0.1, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            # '0.2': MinHashLSH(\n",
    "            #     threshold=0.2, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            # '0.3': MinHashLSH(\n",
    "            #     threshold=0.3, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            # '0.4': MinHashLSH(\n",
    "            #     threshold=0.4, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            # '0.5': MinHashLSH(\n",
    "            #     threshold=0.5, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            # '0.6': MinHashLSH(\n",
    "            #     threshold=0.6, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            # '0.7': MinHashLSH(\n",
    "            #     threshold=0.7, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            '0.8': MinHashLSH(\n",
    "                threshold=0.8, num_perm=128, storage_config={\n",
    "                'type': 'redis',\n",
    "                'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "                'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "             })#,\n",
    "            # '0.9': MinHashLSH(\n",
    "            #     threshold=0.9, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            # }),\n",
    "            # '1.0': MinHashLSH(\n",
    "            #     threshold=1.0, num_perm=128, storage_config={\n",
    "            #     'type': 'redis',\n",
    "            #     'basename': bytearray(self.index_signature[env], 'utf-8'),\n",
    "            #     'redis': {'host': Config.cache_host, 'port': Config.cache_port, 'db': 3}\n",
    "            #  })\n",
    "        }\n",
    "\n",
    "    \n",
    "    # TODO: refactor, adding delete x env amd clearing diff. between namespace and db\n",
    "    def reset_storage_system(self, verbose=True):\n",
    "        self._clear_cache_ns('db0')\n",
    "        self._clear_cache_ns('db1')\n",
    "        self._clear_cache_ns('db2')\n",
    "        self._clear_cache_ns('db3')\n",
    "\n",
    "        self.search_engine.clear_es()\n",
    "        self.search_engine.create_es_schema_content()\n",
    "        self.search_engine.create_es_schema_signature()\n",
    "\n",
    "        self.graph_db.delete_all()\n",
    "\n",
    "        if (verbose): print(\"system storage reset completed\")\n",
    "\n",
    "    def create_profiles(self, config_data, verbose=True):\n",
    "        profiles = {\n",
    "            'signature':[],\n",
    "            'content': []\n",
    "        }\n",
    "        \n",
    "        for dataset in config_data:\n",
    "            doc = self._create_dataset_signature(dataset['source'], dataset['schema'], dataset['dataset'], dataset['dataframe'])\n",
    "            profiles['signature'].append(doc)\n",
    "        if (verbose): print(\"created signature profiles\")\n",
    "\n",
    "        for dataset in config_data:\n",
    "            doc = self._create_attributes_signature(dataset['source'], dataset['schema'], dataset['dataset'], dataset['dataframe'], dataset['attributes_blacklist'], dataset['keywords_whitelist'])\n",
    "            profiles['content'].append(doc)\n",
    "        if (verbose): print(\"created content profiles\")\n",
    "        \n",
    "        return profiles\n",
    "    \n",
    "    \n",
    "    def _create_attributes_signature(self, source, schema, dataset, df, attributes_blacklist, keywords_whitelist):\n",
    "        doc, min_hashes = self._calc_attributes_signature(source, schema, dataset, df, attributes_blacklist, keywords_whitelist)\n",
    "        \n",
    "        try:\n",
    "            self._insert_min_hashes(min_hashes, self.lsh)\n",
    "            self.search_engine.upsert(self.index_content[self.env], Utils.calc_idx_content, doc)\n",
    "        except Exception as e:\n",
    "            print(\"something didn't work\", doc, \"\\n\", str(e))\n",
    "        \n",
    "        return doc\n",
    "      \n",
    "    \n",
    "    def _create_dataset_signature(self, source, schema, dataset, df):\n",
    "        doc, min_hashes = self._calc_dataset_signature(source, schema, dataset, df)\n",
    "        \n",
    "        try:\n",
    "            self._insert_min_hashes(min_hashes, self.lsh_sig)\n",
    "            self.search_engine.upsert(self.index_signature[self.env], Utils.calc_idx_signature, doc)\n",
    "        except Exception as e:\n",
    "            print(\"something didn't work\", doc, \"\\n\", str(e))\n",
    "        \n",
    "        return doc\n",
    "\n",
    "    \n",
    "    def _insert_min_hashes(self, min_hashes, lsh_list):\n",
    "        for threshold, lsh in lsh_list.items():\n",
    "            with lsh.insertion_session() as session:\n",
    "                for key, minhash in min_hashes:\n",
    "                    label = key\n",
    "                    if self.debug: print(label)\n",
    "                    if self.debug: print(minhash)\n",
    "                    try:\n",
    "                        lsh.remove(label)\n",
    "                    except:\n",
    "                        if self.debug:  print(\"Key %s doesn't exist.\"%label) \n",
    "                    session.insert(label, minhash)\n",
    "                    \n",
    "    \n",
    "    def _calc_attributes_signature(self, source, schema, dataset, df, attributes_blacklist, keywords_whitelist):\n",
    "        doc = []\n",
    "        min_hashes = []\n",
    "\n",
    "        for c_n in df.columns:\n",
    "            if c_n not in attributes_blacklist:\n",
    "                m = MinHash(num_perm=128)\n",
    "                df_s = df.select(c_n)\n",
    "                dtype = dict(df.dtypes)[c_n]\n",
    "                crd = df_s.distinct().count()\n",
    "                size = df_s.count()\n",
    "                keywords = []\n",
    "\n",
    "                count_boolean = 0\n",
    "                for iterator in df_s.collect():\n",
    "                    v = iterator[0]\n",
    "\n",
    "                    try:\n",
    "                        if dtype == 'int':\n",
    "                            v = str(v).strip()\n",
    "                            m.update(v.encode('utf8'))\n",
    "                        elif dtype == 'bool':\n",
    "                            # do nothing\n",
    "                            #m.update((str(v)).encode('utf8'))\n",
    "                            count_boolean = count_boolean + 1 \n",
    "                            if count_boolean == 1: print(\"%s: %s\", dtype,str(v)) \n",
    "                        elif dtype == 'date': \n",
    "                            v = v.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                            m.update(v.encode('utf8'))\n",
    "                        elif (dtype == 'double'):\n",
    "                            m.update((str(v)).encode('utf8'))\n",
    "                        else:\n",
    "                            if v is not None:\n",
    "                                m.update(str(v).encode('utf8'))\n",
    "\n",
    "                        if  c_n in keywords_whitelist and v is not None and v != '[Null]' :\n",
    "                            keywords.append(v)\n",
    "                    except Exception as e:\n",
    "                        pprint(e)\n",
    "                        print(\"%s: %s\", dtype,str(v))\n",
    "\n",
    "                data = {}\n",
    "                data['source'] = source\n",
    "                data['schema'] = schema\n",
    "                data['dataset'] = dataset\n",
    "                data['attribute'] = c_n\n",
    "                data['data_type'] = dtype\n",
    "                data['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                data['size'] = size\n",
    "                data['cardinality'] = crd\n",
    "                data['uniqueness'] = crd / size     \n",
    "                data['min_hash'] =  m.hashvalues\n",
    "                data['min_hash_seed'] = m.seed\n",
    "                data['keywords'] = ' '.join(keywords)\n",
    "\n",
    "                min_hashes.append((Utils.calc_idx_content(data), m))\n",
    "                doc.append(data)\n",
    "\n",
    "        return doc, min_hashes\n",
    "\n",
    "\n",
    "    def _calc_dataset_signature(self, source, schema, dataset, df):\n",
    "        min_hashes = []\n",
    "        keywords = []\n",
    "\n",
    "        m = MinHash(num_perm=128)\n",
    "        for c in df.columns:\n",
    "            m.update(c.encode('utf8'))\n",
    "            keywords.append(c)\n",
    "\n",
    "        data = {}\n",
    "        data['source'] = source\n",
    "        data['schema'] = schema\n",
    "        data['dataset'] = dataset\n",
    "        data['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        data['min_hash'] =  m.hashvalues\n",
    "        data['min_hash_seed'] = m.seed\n",
    "        data['data_types'] = Utils.serialize_dict_data_types(Utils.get_df_data_types_as_dict(df))\n",
    "        data['keywords'] = ' '.join(keywords)\n",
    "\n",
    "        min_hashes.append((Utils.calc_idx_signature(data), m))\n",
    "\n",
    "        data['version'] = self._calc_version(Utils.calc_idx_signature(data), Utils.get_df_data_types_as_dict(df))\n",
    "\n",
    "        return [data], min_hashes\n",
    "\n",
    "    \n",
    "    def _calc_version(self, k, d):\n",
    "        h = Utils.dict_hash(d)\n",
    "        v = self.cache.get(k)\n",
    "        version = 0\n",
    "\n",
    "        if self.debug: print(v)\n",
    "        if v:\n",
    "            ser = json.loads(v)\n",
    "            if self.debug: print(\"%s: %s\", ser, k)\n",
    "            version = int(ser['version'])\n",
    "            if ser['hash'] != h:\n",
    "                version = version + 1\n",
    "                if self.debug: print('Version incremented for key: %s' % k)\\\n",
    "\n",
    "        nd = {\n",
    "            'hash': h,\n",
    "            'version': version\n",
    "        }\n",
    "\n",
    "        self.cache.set(k, json.dumps(nd))\n",
    "\n",
    "        return version\n",
    "\n",
    "\n",
    "    def update_graph(self, verbose=True):     \n",
    "        # Create Table nodes and their relations - if something has changed, a new node is added\n",
    "        hits = self.search_engine.get_hits(self.index_signature[self.env])\n",
    "        edges = []\n",
    "\n",
    "        for hit in hits:\n",
    "            min_hash_hashes = np.array(hit['_source'][\"min_hash\"],  dtype=np.int64)\n",
    "            min_hash_seed = hit['_source'][\"min_hash_seed\"]\n",
    "\n",
    "            mt = MinHash(num_perm=128)\n",
    "            mt.hashvalues = min_hash_hashes\n",
    "            mt.seed = min_hash_seed\n",
    "\n",
    "            for threshold, lsh in self.lsh_sig.items():\n",
    "                bucket = lsh.query(mt)\n",
    "                edges.append((hit['_id'], bucket, str(threshold)))\n",
    "        \n",
    "        for hit in hits: self.graph_db.insert_table_node(hit['_id'], hit['_source'])\n",
    "        for k,b,th in edges:\n",
    "            for v in b: \n",
    "                if self.debug: print(\"key: %s - bucket: %s - threshold: %s\"%(k, b, th))\n",
    "                self.graph_db.add_table_table_relation(k,v,th)\n",
    "\n",
    "        if (verbose): print(\"created dataset nodes and their relationships\")\n",
    "\n",
    "        # Create Column nodes and their relations - if something has changed, a new node is added\n",
    "        hits = self.search_engine.get_hits(self.index_content[self.env])\n",
    "        \n",
    "        edges = []\n",
    "        for hit in hits:\n",
    "            min_hash_hashes = np.array(hit['_source'][\"min_hash\"],  dtype=np.int64)\n",
    "            min_hash_seed = hit['_source'][\"min_hash_seed\"]\n",
    "\n",
    "            mt = MinHash(num_perm=128)\n",
    "            mt.hashvalues = min_hash_hashes\n",
    "            mt.seed = min_hash_seed\n",
    "\n",
    "            for threshold, lsh in self.lsh.items():\n",
    "                bucket = lsh.query(mt)\n",
    "                edges.append((hit['_id'], bucket, str(threshold)))\n",
    "        \n",
    "        for hit in hits: self.graph_db.insert_column_node(hit['_id'], Utils.calc_idx_signature(hit['_source']), hit['_source'])\n",
    "        for k,b,th in edges:\n",
    "            for v in b: \n",
    "                if self.debug: print(\"key: %s - bucket: %s - threshold: %s\"%(k, b, th))\n",
    "                self.graph_db.add_column_column_relation(k,v,th)\n",
    "\n",
    "        if (verbose): print(\"created attribute nodes and their relationships\")\n",
    "\n",
    "        # Cleanup\n",
    "        self.graph_db.remove_self_relations()\n",
    "        self.graph_db.close()\n",
    "\n",
    "        if (verbose): print(\"cleaned up\")\n",
    "    \n",
    "    def _clear_cache_ns(self, ns):\n",
    "        \"\"\"\n",
    "        Clears a namespace in redis cache.\n",
    "        This may be very time consuming.\n",
    "        :param ns: str, namespace i.e your:prefix*\n",
    "        :return: int, num cleared keys\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        pipe = self.cache.pipeline()\n",
    "        for key in self.cache.scan_iter(ns):\n",
    "            pipe.delete(key)\n",
    "            count += 1\n",
    "        pipe.execute()\n",
    "        return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9012210-3620-4d9e-b386-72168e4947e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDescriptionService:\n",
    "    \n",
    "    class DataDescriptor(Document):\n",
    "        metadata = DictField()\n",
    "        signature = StringField(required=True, max_length=300)\n",
    "        description = StringField(required=True)\n",
    "        hdfs_path = StringField(required=True, max_length=255)\n",
    "        source = StringField(required=True, max_length=255)\n",
    "        schema = StringField(required=True, max_length=255)\n",
    "        dataset = StringField(required=True, max_length=255)\n",
    "        created_at = StringField(max_length=80)\n",
    "        data_types = DictField()\n",
    "        notes = ListField()\n",
    "    \n",
    "    def __init__(self, db, username, password, host, port, env, content_analyzer_service):\n",
    "        disconnect()\n",
    "        connect(\n",
    "            db=db,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            authentication_source='admin',\n",
    "            host=host, \n",
    "            port=port\n",
    "        )\n",
    "        self.env = env\n",
    "        self.content_analyzer_service = content_analyzer_service\n",
    "\n",
    "    \n",
    "    def clear_all(self):\n",
    "        DataDescriptionService.DataDescriptor.drop_collection()\n",
    "    \n",
    "    \n",
    "    def create_data_descriptions(self, data_to_load, verbose=True):\n",
    "        for data in data_to_load:\n",
    "            hdfs_path = data['hdfs_path']\n",
    "            signature = data['signature']\n",
    "            description = data['source_description']\n",
    "            metadata = self.content_analyzer_service.create_metadata(hdfs_path)\n",
    "            document = DataDescriptionService.DataDescriptor(\n",
    "                metadata = metadata,\n",
    "                signature = data['signature'],\n",
    "                description = data['source_description'],\n",
    "                source = data['source'],\n",
    "                schema = data['schema'],\n",
    "                dataset = data['dataset'],\n",
    "                hdfs_path = data['hdfs_path']\n",
    "            )\n",
    "            document.save()\n",
    "        \n",
    "        if (verbose): print('created data descriptions')\n",
    "    \n",
    "    \n",
    "    def add_additional_data(self, profiles, verbose=True):\n",
    "        for profile in profiles['signature']:\n",
    "            data = profile[0]\n",
    "            signature = Utils.calc_idx_signature(data)\n",
    "            for doc in DataDescriptionService.DataDescriptor.objects(signature=signature):\n",
    "                doc.created_at = data['timestamp']\n",
    "                doc.data_types = Utils.deserialize_dict_data_types(data['data_types'])\n",
    "                doc.save()\n",
    "        if (verbose): print('added additional data to existing data descriptions')\n",
    "        \n",
    "        \n",
    "    def find_by_source(self, source):\n",
    "        return DataDescriptionService.DataDescriptor.objects(source=source)\n",
    "    \n",
    "    \n",
    "    def find_by_schema(self, schema):\n",
    "        return DataDescriptionService.DataDescriptor.objects(schema=schema)\n",
    "    \n",
    "    \n",
    "    def find_by_dataset(self, dataset):\n",
    "        return DataDescriptionService.DataDescriptor.objects(dataset=dataset)\n",
    "    \n",
    "    def find_by_signature(self, signature):\n",
    "        return DataDescriptionService.DataDescriptor.objects(signature=signature)\n",
    "    \n",
    "    def update_description(self, signature, description):\n",
    "        for doc in DataDescriptionService.DataDescriptor.objects(signature=signature):\n",
    "            doc.description = description\n",
    "            doc.save()\n",
    "    \n",
    "    def add_note(self, signature, note):\n",
    "        for doc in DataDescriptionService.DataDescriptor.objects(signature=signature):\n",
    "            notes = doc.notes\n",
    "            notes.append(note)\n",
    "            doc.notes = notes\n",
    "            doc.save()\n",
    "    \n",
    "    def toDict(self, dataset):\n",
    "        return {\n",
    "            'metadata': dataset.metadata,\n",
    "            'signature': dataset.signature,\n",
    "            'description': dataset.description,\n",
    "            'hdfs_path': dataset.hdfs_path,\n",
    "            'source': dataset.source,\n",
    "            'schema': dataset.schema,\n",
    "            'dataset': dataset.dataset,\n",
    "            'created_at': dataset.created_at,\n",
    "            'data_types': dataset.data_types,\n",
    "            'notes': dataset.notes\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfabc7b-5c2b-42df-9a73-26ce8d0ed99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentAnalyzerService:\n",
    "    op_open = 'op=OPEN'\n",
    "    \n",
    "    def create_metadata(self, path):\n",
    "        data = parser.from_file(Config.content_analyzer_url_base + path + '?' + ContentAnalyzerService.op_open)\n",
    "        return data['metadata']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a356fa9-6c39-4017-ad0d-da89f8c942df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExplorationService: \n",
    "    def __init__(self, graph_service, search_service, data_description_service, data_to_load, debug=False):\n",
    "        self.debug = debug\n",
    "        self.graph_service = graph_service\n",
    "        self.search_service = search_service\n",
    "        self.data_description_service = data_description_service\n",
    "        self.data_to_load = data_to_load \n",
    "        \n",
    "    @staticmethod\n",
    "    def ts_to_str(ts):\n",
    "        date_time = datetime.fromtimestamp(int(ts / 1000))\n",
    "        d = date_time.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "        return d\n",
    "\n",
    "    @staticmethod\n",
    "    def related_nodes_by_content_to_str(node):\n",
    "         return (str(node[0]['dataset']) + ' version: ' \\\n",
    "                 + str(node[0]['version']) + ' -> (' + str(node[1]['attribute']) \\\n",
    "                 + ' - ' + str(node[2]['attribute']) + ') <- ' \\\n",
    "                 + str(node[3]['dataset']) + ' version: ' \\\n",
    "                 + str(node[3]['version']) + '\\n')\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_ds_attribute_node(d):\n",
    "        return {\n",
    "            'idx': d[0],\n",
    "            'source': d[1],\n",
    "            'schema': d[2],\n",
    "            'dataset': d[3],\n",
    "            'attribute': d[4],\n",
    "            'version':  d[5]\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def create_ds_dataset_node(d):\n",
    "        return {\n",
    "            'idx': d[0],\n",
    "            'source': d[1],\n",
    "            'schema': d[2],\n",
    "            'dataset': d[3],\n",
    "            'version':  d[4],\n",
    "            'uri': ''\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_pd_dataframe_of_results(queries):\n",
    "        sources_datasets = [i['source_name'] for i in queries.values()]\n",
    "        sources_attributes = [i['source_attribute_name'] for i in queries.values()]\n",
    "        targets_datasets = [i['target_name'] for i in queries.values()]\n",
    "        targets_attributes = [i['target_attribute_name'] for i in queries.values()]\n",
    "\n",
    "        table_q = pd.DataFrame(zip(queries.keys(), sources_datasets, sources_attributes, targets_attributes, targets_datasets), columns=[\"Query\", \"Dataset a\", \"Attribute a\", \"Attribute b\", \"Dataset b\"])\n",
    "        table_oq = pd.DataFrame(zip(sources_datasets, sources_attributes, targets_attributes, targets_datasets), columns=[\"Dataset a\", \"Attribute a\", \"Attribute b\", \"Dataset b\"])\n",
    "\n",
    "        return table_q, table_oq\n",
    "    \n",
    "    \n",
    "    def find_related_datasets_by_signature(self, dataset, treshold, dastart, daend):\n",
    "        related_nodes = []\n",
    "\n",
    "        related_nodes_by_signature_to_2_grade = self.graph_service.find_related_tables_by_signature_to_2_grade(dataset, treshold, dastart, daend)\n",
    "        for node in related_nodes_by_signature_to_2_grade: \n",
    "            if node[2] is not None:\n",
    "                s_grad_dataset =  (node[2]['idx'], node[2]['source'], node[2]['schema'], node[2]['dataset'], node[2]['version'])\n",
    "            else:\n",
    "                s_grad_dataset = ()\n",
    "\n",
    "            related_nodes.append({\n",
    "                'source_dataset': (node[0]['idx'], node[0]['source'], node[0]['schema'], node[0]['dataset'], node[0]['version']),\n",
    "                'f_grad_dataset': (node[1]['idx'], node[1]['source'], node[1]['schema'], node[1]['dataset'], node[1]['version']),\n",
    "                's_grad_dataset': s_grad_dataset\n",
    "            })\n",
    "\n",
    "        return related_nodes\n",
    "\n",
    "\n",
    "    def find_related_datasets_by_content(self, dataset, treshold, dastart, daend):\n",
    "        related_nodes = []\n",
    "\n",
    "        related_nodes_by_content_to_1_grad = self.graph_service.find_related_tables_by_content_to_1_grad(dataset, treshold, dastart, daend)\n",
    "\n",
    "        for node in related_nodes_by_content_to_1_grad: \n",
    "            related_nodes.append({\n",
    "                'source_dataset': (node[0]['idx'], node[0]['source'], node[0]['schema'], node[0]['dataset'], node[0]['version']),\n",
    "                'source_attribute': (node[1]['idx'], node[1]['source'], node[0]['schema'], node[1]['dataset'], node[1]['attribute'], node[1]['version']),\n",
    "                'target_attribute': (node[2]['idx'], node[2]['source'], node[2]['schema'], node[2]['dataset'], node[2]['attribute'], node[2]['version']),\n",
    "                'target_dataset': (node[3]['idx'], node[3]['source'], node[3]['schema'], node[3]['dataset'], node[3]['version'])\n",
    "            })\n",
    "            if self.debug: print(DataExplorationService.related_nodes_by_content_to_str(node))\n",
    "\n",
    "        return related_nodes\n",
    "\n",
    "        \n",
    "    def discover_datasets(self, topic, dastart, daend, treshold_signature='0.9', treshold_content='0.7', verbose=True):\n",
    "        if (verbose): print('Dataset Discovery for topic %s, from date %s to date %s\\n\\n' % (topic, DataExplorationService.ts_to_str(dastart), DataExplorationService.ts_to_str(daend)))\n",
    "\n",
    "        # find datasets for topic over keywords with elasticsearch\n",
    "        hits_for_topic = self.search_service.search_by_keyword(Config.index_content[env], topic)\n",
    "\n",
    "        # find related datasets with neo4j\n",
    "        results = []\n",
    "        for hit in hits_for_topic:\n",
    "            related_nodes_by_signature = self.find_related_datasets_by_signature(hit['dataset'], treshold_signature, dastart, daend)            \n",
    "            related_nodes_by_content = self.find_related_datasets_by_content(hit['attribute'], treshold_content, dastart, daend)\n",
    "\n",
    "            results.append({'hit': hit,\n",
    "                            'related_nodes_by_signature': related_nodes_by_signature,\n",
    "                            'related_nodes_by_content': related_nodes_by_content\n",
    "                           })\n",
    "        return results\n",
    "\n",
    "    \n",
    "    def explore_by(self, attribute_type, attribute):\n",
    "        if attribute_type == 'source':\n",
    "            datasets = self.data_description_service.find_by_source(attribute)\n",
    "        elif attribute_type == 'schema': \n",
    "            datasets = self.data_description_service.find_by_schema(attribute)\n",
    "        elif attribute_type == 'dataset': \n",
    "            datasets = self.data_description_service.find_by_dataset(attribute)\n",
    "        else:\n",
    "            print('type not known')      \n",
    "        return datasets\n",
    "\n",
    "\n",
    "    def hydtrate_exploration_results(self, datasets, treshold_signature, treshold_content, dastart, daend):\n",
    "        results = []\n",
    "\n",
    "        for dataset in datasets:\n",
    "            related_nodes_by_signature = self.find_related_datasets_by_signature(dataset.dataset, treshold_signature, dastart, daend)            \n",
    "\n",
    "            related_nodes_by_content = []\n",
    "\n",
    "            for attribute in dataset.data_types.keys():\n",
    "                related_nodes = self.find_related_datasets_by_content(attribute, treshold_content, dastart, daend)\n",
    "                if len(related_nodes) > 0:\n",
    "                    related_nodes_by_content.append(related_nodes)\n",
    "\n",
    "            results.append({'dataset': dataset.to_json(), #self.data_description_service.toDict(dataset),\n",
    "                            'related_nodes_by_signature': related_nodes_by_signature,\n",
    "                            'related_nodes_by_content': related_nodes_by_content\n",
    "                           })\n",
    "        return results\n",
    "\n",
    "\n",
    "    def explore_by_source(self, source, treshold_signature, treshold_content, dastart, daend):\n",
    "        datasets = self.explore_by('source', source)\n",
    "        results = self.hydtrate_exploration_results(datasets, treshold_signature, treshold_content, dastart, daend)\n",
    "        return results\n",
    "\n",
    "\n",
    "    def explore_by_schema(self, schema, treshold_signature, treshold_content, dastart, daend):\n",
    "        datasets = self.explore_by('schema', schema)\n",
    "        results = self.hydtrate_exploration_results(datasets, treshold_signature, treshold_content, dastart, daend)\n",
    "        return results\n",
    "\n",
    "\n",
    "    def explore_by_dataset(self, dataset, treshold_signature, treshold_content, dastart, daend):\n",
    "        datasets = self.explore_by('dataset', dataset)\n",
    "        results = self.hydtrate_exploration_results(datasets, treshold_signature, treshold_content, dastart, daend)\n",
    "        return results\n",
    "\n",
    "\n",
    "    def explore_by_signature(self, signature):\n",
    "            return self.data_description_service.find_by_signature(signature)\n",
    "\n",
    "\n",
    "    def insert_note(self, signature, note):\n",
    "        self.data_description_service.add_note(signature, note)\n",
    "\n",
    "\n",
    "    def update_description(self, signature, description):\n",
    "        self.data_description_service.update_description(signature, description)\n",
    "\n",
    "    def generate_queries_for_discovered_datasets(self, discovered_datasets):\n",
    "        queries = {}\n",
    "        for hit in discovered_datasets:\n",
    "            for related_node_by_content in hit['related_nodes_by_content']:    \n",
    "                data_x_q = {\n",
    "                    'source': DataExplorationService.create_ds_dataset_node(related_node_by_content['source_dataset']),\n",
    "                    'join_a_s': DataExplorationService.create_ds_attribute_node(related_node_by_content['source_attribute']), \n",
    "                    'join_a_t': DataExplorationService.create_ds_attribute_node(related_node_by_content['target_attribute']), \n",
    "                    'target': DataExplorationService.create_ds_dataset_node(related_node_by_content['target_dataset']),\n",
    "                }\n",
    "\n",
    "                ## TODO: questo Ã¨ un errore: i path devono essere persistiti nei metadata\n",
    "                idx_source = Utils.calc_idx_resource(data_x_q['source']['source'], data_x_q['source']['schema'], data_x_q['source']['dataset'])\n",
    "                hdfs_uri_source = next(item['hdfs_path'] for item in self.data_to_load if item['signature'] == idx_source)\n",
    "                data_x_q['source']['uri'] = Config.hadoop_uri + hdfs_uri_source\n",
    "\n",
    "                idx_target = Utils.calc_idx_resource(data_x_q['target']['source'], data_x_q['target']['schema'], data_x_q['target']['dataset'])\n",
    "                hdfs_uri_target = next(item['hdfs_path'] for item in self.data_to_load if item['signature'] == idx_target)\n",
    "                data_x_q['target']['uri'] = Config.hadoop_uri + hdfs_uri_target\n",
    "\n",
    "                q = f'''\n",
    "                    SELECT * FROM {data_x_q['source']['dataset']} a JOIN {data_x_q['target']['dataset']} b ON (a.{data_x_q['join_a_s']['attribute']} = b.{data_x_q['join_a_t']['attribute']})\n",
    "                '''\n",
    "\n",
    "                queries[q.strip()] = {\n",
    "                        'source_name': data_x_q['source']['dataset'].strip(),\n",
    "                        'source_uri': data_x_q['source']['uri'].strip(),\n",
    "                        'source_attribute_name': data_x_q['join_a_s']['attribute'].strip(),\n",
    "                        'target_name': data_x_q['target']['dataset'].strip(),\n",
    "                        'target_uri': data_x_q['target']['uri'].strip(),\n",
    "                        'target_attribute_name': data_x_q['join_a_t']['attribute'].strip()\n",
    "                }\n",
    "\n",
    "        return queries\n",
    "    \n",
    "    \n",
    "    def exec_query_by_number(self, n):\n",
    "        query = list(queries.keys())[n]\n",
    "        return self.exec_query(query, queries[query])\n",
    "    \n",
    "    def exec_query(self, query, context):\n",
    "        # if self.debug: print(f'{queries[i]['source_name']} {queries[i]['source_uri']} {queries[i]['target_name']} {queries[i]['target_uri']}')\n",
    "\n",
    "        df_source = spark.read.format('csv').option('header', 'true').load(context['source_uri'])\n",
    "        df_target = spark.read.format('csv').option('header', 'true').load(context['target_uri'])\n",
    "\n",
    "        df_source.createOrReplaceTempView(context['source_name'])\n",
    "        df_target.createOrReplaceTempView(context['target_name'])\n",
    "\n",
    "        if self.debug: df_source.show()\n",
    "        if self.debug: df_target.show()\n",
    "\n",
    "        df_res = None\n",
    "        try:\n",
    "            pprint(query)\n",
    "            df_res = spark.sql(query)\n",
    "        except:\n",
    "            print('Something didn\\'t work executing this query: %s ' % query)\n",
    "\n",
    "        return df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee3994cd-b149-49fd-9747-2a5f8b1b7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_experiment_infra(list_to_read, env, reset_on_restart, debug):\n",
    "    if (reset_on_restart):\n",
    "        mainteinance_service.reset_storage_system()\n",
    "        data_description_service.clear_all()\n",
    "    \n",
    "    data_description_service.create_data_descriptions(data_to_load)\n",
    "    profiles = mainteinance_service.create_profiles(data_to_load)\n",
    "    data_description_service.add_additional_data(profiles)\n",
    "    mainteinance_service.update_graph()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf5a18-2aaa-49ff-bdf1-a7ff63fa49e5",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Die Dataingestion wird an dieser Stelle simultiert, da sie nicht im Fokus der Arbeit steht. \n",
    "\n",
    "Das CSV Format wird als gegeben, da in der Arbeit erstmal nur tabellarische Datenstrukturen konsumiert werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ad88665-6f00-46b3-8781-1f566f15275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/address__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/address__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/addresstype__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/contacttype__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/businessentity__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/businessentityaddress__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/businessentitycontact__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/countryregion__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/emailaddress__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/personphone__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/password__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/person__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/vadditionalcontactinfo__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/phonenumbertype__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/.ipynb_checkpoints/stateprovince__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/address_2__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/addresstype__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/contacttype__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/businessentity__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/businessentityaddress__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/businessentitycontact__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/countryregion__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/emailaddress__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/personphone__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/password__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/person__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/vadditionalcontactinfo__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/phonenumbertype__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/stateprovince__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/adventureworks-for-postgres/person/address__2022-11-23.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/hr-dataset/hrd/HRDataset_generated__2022-11-20.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/hr-dataset/hrd/.ipynb_checkpoints/HRDataset_generated__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/hr-dataset/hrd/.ipynb_checkpoints/HRDataset_v14__2022-11-20-checkpoint.csv': File exists\n",
      "\n",
      "\n",
      "copyFromLocal: `/sample-data/hr-dataset/hrd/HRDataset_v14__2022-11-20.csv': File exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyFromLocal ./sample-data/ /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d9c16-ea09-46d2-b897-ceab15c4ca63",
   "metadata": {},
   "source": [
    "Danach wird eine Pull Operation auf einen Kafka Topic simuliert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "103758a4-d6ff-4fbd-b1bc-d80ddff1496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_read = [\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'address',\n",
    "        'filename': 'address__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'address_2',\n",
    "        'filename': 'address_2__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'addresstype',\n",
    "        'filename': 'addresstype__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'businessentity',\n",
    "        'filename': 'businessentity__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'businessentityaddress',\n",
    "        'filename': 'businessentityaddress__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'contacttype',\n",
    "        'filename': 'contacttype__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'countryregion',\n",
    "        'filename': 'countryregion__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'emailaddress',\n",
    "        'filename': 'emailaddress__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'password',\n",
    "        'filename': 'password__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'person',\n",
    "        'filename': 'person__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'personphone',\n",
    "        'filename': 'personphone__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'phonenumbertype',\n",
    "        'filename': 'phonenumbertype__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'stateprovince',\n",
    "        'filename': 'stateprovince__2022-11-20.csv'\n",
    "    },\n",
    "    {\n",
    "        'source': 'adventureworks-for-postgres',\n",
    "        'schema': 'person',\n",
    "        'dataset': 'vadditionalcontactinfo',\n",
    "        'filename': 'vadditionalcontactinfo__2022-11-20.csv'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9362375-338b-4c6b-9d72-75768cc7f062",
   "metadata": {},
   "source": [
    "## Lesen der Konfigurationfiles und generierung einer entsprechenden Datenzustruktur zur Steuerung der Data Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c24e2a26-c678-4da4-980a-a759667981e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'dev'\n",
    "debug = False\n",
    "data_to_load = ConfigLoader.data_to_load_job(list_to_read, ConfigLoader.load_config('havel-config.yaml'))\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "search_service = SearchService(Config.search_service_host, Config.search_service_user, Config.search_service_password, env, Config.index_signature, Config.index_content, debug)\n",
    "graph_service = GraphService(Config.graph_service_host, Config.graph_service_user, Config.graph_service_password, env, debug)\n",
    "cache = redis.Redis(connection_pool= redis.ConnectionPool(host=Config.cache_host, port=Config.cache_port, db=Config.cache_db))\n",
    "mainteinance_service = MainteinanceService(env, graph_service, search_service, cache, Config.index_signature, Config.index_content, debug)\n",
    "data_description_service = DataDescriptionService(\n",
    "    db = Config.data_description_service_host_db_prefix + '-'+ env,\n",
    "    username = Config.data_description_service_username,\n",
    "    password = Config.data_description_service_password,\n",
    "    host = Config.data_description_service_host, \n",
    "    port = Config.data_description_service_port,\n",
    "    env = env,\n",
    "    content_analyzer_service = ContentAnalyzerService()\n",
    ")\n",
    "data_exploration_service = DataExplorationService(graph_service=graph_service, search_service=search_service, data_description_service=data_description_service, data_to_load=data_to_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c874cbf-bbcd-42ca-8138-8a643c2e98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_experiment_infra(list_to_read, env=env, reset_on_restart=True, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a515a-f427-4db8-974b-db58f0290180",
   "metadata": {},
   "source": [
    "## Data Discovery\n",
    "\n",
    "## Data Exploration\n",
    "Dazu gibt es zwei MÃ¶glichkeiten, ein Benutzer kann:\n",
    "- entweder den Katalog der vorhandenen Quellen und DatensÃ¤tze durchsuchen\n",
    "- oder eine Stichwortsuche durchfÃ¼hren.\n",
    "\n",
    "FÃ¼r jeden identifizierten Datensatz werden:\n",
    "- das Schema\n",
    "- eine Beschreibung\n",
    "- Statistiken\n",
    "- die Liste der verwandten DatensÃ¤tze nach ÃhnlichkeitsverhÃ¤ltnis bereitgestellt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280fbe3-485e-4343-8c34-613e47939e42",
   "metadata": {},
   "source": [
    "## Discover Datasets by keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3288d05a-8399-43d9-b9b2-5162f57f28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_experiment(topic, dastart, daend, treshold_signature, treshold_content):\n",
    "    discovered_datasets = data_exploration_service.discover_datasets(topic=topic, dastart=dastart, daend=daend, treshold_signature=treshold_signature, treshold_content=treshold_content)\n",
    "    queries = data_exploration_service.generate_queries_for_discovered_datasets(discovered_datasets)\n",
    "    results_dataframe_q, results_dataframe_oq = data_exploration_service.get_pd_dataframe_of_results(queries)\n",
    "    display(results_dataframe_q)\n",
    "    display(results_dataframe_oq)\n",
    "    return results_dataframe_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ac80d92c-54fe-48fe-9a68-3ac55155db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Discovery for topic Shoop, from date 01/29/2022, 12:12:12 to date 02/17/2023, 12:23:07\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Dataset a</th>\n",
       "      <th>Attribute a</th>\n",
       "      <th>Attribute b</th>\n",
       "      <th>Dataset b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * FROM person a JOIN password b ON (a.b...</td>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * FROM person a JOIN emailaddress b ON ...</td>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>emailaddressid</td>\n",
       "      <td>emailaddress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT * FROM person a JOIN businessentityaddr...</td>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityaddress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT * FROM person a JOIN personphone b ON (...</td>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>personphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT * FROM person a JOIN emailaddress b ON ...</td>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>emailaddress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SELECT * FROM person a JOIN businessentity b O...</td>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SELECT * FROM person a JOIN vadditionalcontact...</td>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>vadditionalcontactinfo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SELECT * FROM vadditionalcontactinfo a JOIN pe...</td>\n",
       "      <td>vadditionalcontactinfo</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query               Dataset a       Attribute a       Attribute b               Dataset b\n",
       "0  SELECT * FROM person a JOIN password b ON (a.b...                  person  businessentityid  businessentityid                password\n",
       "1  SELECT * FROM person a JOIN emailaddress b ON ...                  person  businessentityid    emailaddressid            emailaddress\n",
       "2  SELECT * FROM person a JOIN businessentityaddr...                  person  businessentityid  businessentityid   businessentityaddress\n",
       "3  SELECT * FROM person a JOIN personphone b ON (...                  person  businessentityid  businessentityid             personphone\n",
       "4  SELECT * FROM person a JOIN emailaddress b ON ...                  person  businessentityid  businessentityid            emailaddress\n",
       "5  SELECT * FROM person a JOIN businessentity b O...                  person  businessentityid  businessentityid          businessentity\n",
       "6  SELECT * FROM person a JOIN vadditionalcontact...                  person  businessentityid  businessentityid  vadditionalcontactinfo\n",
       "7  SELECT * FROM vadditionalcontactinfo a JOIN pe...  vadditionalcontactinfo  businessentityid  businessentityid                  person"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset a</th>\n",
       "      <th>Attribute a</th>\n",
       "      <th>Attribute b</th>\n",
       "      <th>Dataset b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>emailaddressid</td>\n",
       "      <td>emailaddress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityaddress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>personphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>emailaddress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>vadditionalcontactinfo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vadditionalcontactinfo</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset a       Attribute a       Attribute b               Dataset b\n",
       "0                  person  businessentityid  businessentityid                password\n",
       "1                  person  businessentityid    emailaddressid            emailaddress\n",
       "2                  person  businessentityid  businessentityid   businessentityaddress\n",
       "3                  person  businessentityid  businessentityid             personphone\n",
       "4                  person  businessentityid  businessentityid            emailaddress\n",
       "5                  person  businessentityid  businessentityid          businessentity\n",
       "6                  person  businessentityid  businessentityid  vadditionalcontactinfo\n",
       "7  vadditionalcontactinfo  businessentityid  businessentityid                  person"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Eminhizer\n",
    "# Firestone Drive\n",
    "topic = \"Shoop\"\n",
    "treshold_signature='0.8'\n",
    "treshold_content='0.5'\n",
    "\n",
    "daend = int(datetime.timestamp(datetime.now())*1000)\n",
    "a = datetime(2022, 1, 29, 12, 12, 12)\n",
    "dastart = int(datetime.timestamp(a)*1000)\n",
    "\n",
    "res_exp_01 = exec_experiment(topic, dastart, daend, treshold_signature, treshold_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "aee81ae6-1008-4e92-b7f1-a0561b348f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Discovery for topic Shoop, from date 01/29/2022, 12:12:12 to date 02/17/2023, 12:23:26\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Dataset a</th>\n",
       "      <th>Attribute a</th>\n",
       "      <th>Attribute b</th>\n",
       "      <th>Dataset b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * FROM person a JOIN vadditionalcontact...</td>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>vadditionalcontactinfo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * FROM vadditionalcontactinfo a JOIN pe...</td>\n",
       "      <td>vadditionalcontactinfo</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query               Dataset a       Attribute a       Attribute b               Dataset b\n",
       "0  SELECT * FROM person a JOIN vadditionalcontact...                  person  businessentityid  businessentityid  vadditionalcontactinfo\n",
       "1  SELECT * FROM vadditionalcontactinfo a JOIN pe...  vadditionalcontactinfo  businessentityid  businessentityid                  person"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset a</th>\n",
       "      <th>Attribute a</th>\n",
       "      <th>Attribute b</th>\n",
       "      <th>Dataset b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>vadditionalcontactinfo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vadditionalcontactinfo</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset a       Attribute a       Attribute b               Dataset b\n",
       "0                  person  businessentityid  businessentityid  vadditionalcontactinfo\n",
       "1  vadditionalcontactinfo  businessentityid  businessentityid                  person"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Eminhizer\n",
    "# Firestone Drive\n",
    "topic = \"Shoop\"\n",
    "\n",
    "daend = int(datetime.timestamp(datetime.now())*1000)\n",
    "a = datetime(2022, 1, 29, 12, 12, 12)\n",
    "dastart = int(datetime.timestamp(a)*1000)\n",
    "treshold_signature='0.9'\n",
    "treshold_content='0.8'\n",
    "\n",
    "res_exp_02 = exec_experiment(topic, dastart, daend, treshold_signature, treshold_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0bee8d4f-e4eb-46b8-af24-440ba2788648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Discovery for topic Eminhizer, from date 01/29/2022, 12:12:12 to date 02/17/2023, 12:23:29\n",
      "\n",
      "\n",
      "('SELECT * FROM person a JOIN emailaddress b ON (a.businessentityid = '\n",
      " 'b.emailaddressid)')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>businessentityid</th>\n",
       "      <th>persontype</th>\n",
       "      <th>namestyle</th>\n",
       "      <th>title</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>suffix</th>\n",
       "      <th>emailpromotion</th>\n",
       "      <th>additionalcontactinfo</th>\n",
       "      <th>demographics</th>\n",
       "      <th>rowguid</th>\n",
       "      <th>modifieddate</th>\n",
       "      <th>businessentityid</th>\n",
       "      <th>emailaddressid</th>\n",
       "      <th>emailaddress</th>\n",
       "      <th>rowguid</th>\n",
       "      <th>modifieddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EM</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>Ken</td>\n",
       "      <td>J</td>\n",
       "      <td>SÃ¡nchez</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[NULL]</td>\n",
       "      <td>\"&lt;IndividualSurvey xmlns=\"\"http://schemas.micr...</td>\n",
       "      <td>92c4279f-1207-48a3-8448-4636514eb7e2</td>\n",
       "      <td>2009-01-07 00:00:00.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ken0@adventure-works.com</td>\n",
       "      <td>8a1901e4-671b-431a-871c-eadb2942e9ee</td>\n",
       "      <td>2009-01-07 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>EM</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>Terri</td>\n",
       "      <td>Lee</td>\n",
       "      <td>Duffy</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>[NULL]</td>\n",
       "      <td>\"&lt;IndividualSurvey xmlns=\"\"http://schemas.micr...</td>\n",
       "      <td>d8763459-8aa8-47cc-aff7-c9079af79033</td>\n",
       "      <td>2008-01-24 00:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>terri0@adventure-works.com</td>\n",
       "      <td>b5ff9efd-72a2-4f87-830b-f338fdd4d162</td>\n",
       "      <td>2008-01-24 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>EM</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>Roberto</td>\n",
       "      <td>None</td>\n",
       "      <td>Tamburello</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[NULL]</td>\n",
       "      <td>\"&lt;IndividualSurvey xmlns=\"\"http://schemas.micr...</td>\n",
       "      <td>e1a2555e-0828-434b-a33b-6f38136a37de</td>\n",
       "      <td>2007-11-04 00:00:00.000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>roberto0@adventure-works.com</td>\n",
       "      <td>c8a51084-1c03-4c58-a8b3-55854ae7c499</td>\n",
       "      <td>2007-11-04 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>EM</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>Rob</td>\n",
       "      <td>None</td>\n",
       "      <td>Walters</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[NULL]</td>\n",
       "      <td>\"&lt;IndividualSurvey xmlns=\"\"http://schemas.micr...</td>\n",
       "      <td>f2d7ce06-38b3-4357-805b-f4b6b71c01ff</td>\n",
       "      <td>2007-11-28 00:00:00.000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>rob0@adventure-works.com</td>\n",
       "      <td>17703ed1-0031-4b4a-afd2-77487a556b3b</td>\n",
       "      <td>2007-11-28 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>EM</td>\n",
       "      <td>false</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Gail</td>\n",
       "      <td>A</td>\n",
       "      <td>Erickson</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[NULL]</td>\n",
       "      <td>\"&lt;IndividualSurvey xmlns=\"\"http://schemas.micr...</td>\n",
       "      <td>f3a3f6b4-ae3b-430c-a754-9f2231ba6fef</td>\n",
       "      <td>2007-12-30 00:00:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>gail0@adventure-works.com</td>\n",
       "      <td>e76d2ea3-08e5-409c-bbe2-5dd1cdf89a3b</td>\n",
       "      <td>2007-12-30 00:00:00.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  businessentityid persontype namestyle title firstname middlename    lastname suffix emailpromotion additionalcontactinfo                                       demographics                               rowguid             modifieddate businessentityid emailaddressid                  emailaddress                               rowguid             modifieddate\n",
       "0                1         EM     false  None       Ken          J     SÃ¡nchez   None              0                [NULL]  \"<IndividualSurvey xmlns=\"\"http://schemas.micr...  92c4279f-1207-48a3-8448-4636514eb7e2  2009-01-07 00:00:00.000                1              1      ken0@adventure-works.com  8a1901e4-671b-431a-871c-eadb2942e9ee  2009-01-07 00:00:00.000\n",
       "1                2         EM     false  None     Terri        Lee       Duffy   None              1                [NULL]  \"<IndividualSurvey xmlns=\"\"http://schemas.micr...  d8763459-8aa8-47cc-aff7-c9079af79033  2008-01-24 00:00:00.000                2              2    terri0@adventure-works.com  b5ff9efd-72a2-4f87-830b-f338fdd4d162  2008-01-24 00:00:00.000\n",
       "2                3         EM     false  None   Roberto       None  Tamburello   None              0                [NULL]  \"<IndividualSurvey xmlns=\"\"http://schemas.micr...  e1a2555e-0828-434b-a33b-6f38136a37de  2007-11-04 00:00:00.000                3              3  roberto0@adventure-works.com  c8a51084-1c03-4c58-a8b3-55854ae7c499  2007-11-04 00:00:00.000\n",
       "3                4         EM     false  None       Rob       None     Walters   None              0                [NULL]  \"<IndividualSurvey xmlns=\"\"http://schemas.micr...  f2d7ce06-38b3-4357-805b-f4b6b71c01ff  2007-11-28 00:00:00.000                4              4      rob0@adventure-works.com  17703ed1-0031-4b4a-afd2-77487a556b3b  2007-11-28 00:00:00.000\n",
       "4                5         EM     false   Ms.      Gail          A    Erickson   None              0                [NULL]  \"<IndividualSurvey xmlns=\"\"http://schemas.micr...  f3a3f6b4-ae3b-430c-a754-9f2231ba6fef  2007-12-30 00:00:00.000                5              5     gail0@adventure-works.com  e76d2ea3-08e5-409c-bbe2-5dd1cdf89a3b  2007-12-30 00:00:00.000"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eminhizer\n",
    "topic = \"Eminhizer\"\n",
    "\n",
    "daend = int(datetime.timestamp(datetime.now())*1000)\n",
    "a = datetime(2022, 1, 29, 12, 12, 12)\n",
    "dastart = int(datetime.timestamp(a)*1000)\n",
    "\n",
    "discovered_datasets = data_exploration_service.discover_datasets(topic=topic, dastart=dastart, daend=daend, treshold_signature='0.9', treshold_content='0.5')\n",
    "queries = data_exploration_service.generate_queries_for_discovered_datasets(discovered_datasets)\n",
    "results_dataframe = data_exploration_service.get_pd_dataframe_of_results(queries)\n",
    "\n",
    "df = data_exploration_service.exec_query_by_number(1)\n",
    "df.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c073c1-ddd7-462b-b3a4-bd17eccd2e98",
   "metadata": {},
   "source": [
    "## Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2a87e186-7266-4126-9d43-eee6819270f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(exploration_results): \n",
    "    df = pd.DataFrame(columns=['dataset','related_nodes_by_signature', 'related_nodes_by_content'], index=[range(len(exploration_results))])\n",
    "    i = 0\n",
    "    for res in exploration_results:\n",
    "        dataset_struct = eval(res['dataset'])\n",
    "        dataset = '%s.%s.%s'%(dataset_struct['source'],dataset_struct['schema'],dataset_struct['dataset'])\n",
    "\n",
    "        related_nodes_by_signature = []\n",
    "        for n in res['related_nodes_by_signature']:\n",
    "            related_nodes_by_signature.append(n['f_grad_dataset'])\n",
    "\n",
    "        related_nodes_by_content = []\n",
    "        for no in res['related_nodes_by_content']:\n",
    "            n = no[0]\n",
    "            n['source_attribute']\n",
    "            s = '%s.%s.%s'%(n['source_attribute'][1],n['source_attribute'][2], n['source_attribute'][3])\n",
    "            t = '%s.%s.%s'%(n['target_attribute'][1],n['target_attribute'][2], n['source_attribute'][3])\n",
    "            rel = '%s -> %s'%(s,t)\n",
    "            related_nodes_by_content.append(rel)\n",
    "\n",
    "        new_row = {'dataset': dataset, 'related_nodes_by_signature':  related_nodes_by_signature, 'related_nodes_by_content': related_nodes_by_content}\n",
    "        df.loc[i] = [dataset, related_nodes_by_signature, related_nodes_by_content]\n",
    "        i = i + 1\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ef4a0e4d-107e-426e-8c70-ee484f170a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>related_nodes_by_signature</th>\n",
       "      <th>related_nodes_by_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adventureworks-for-postgres.person.address</td>\n",
       "      <td>[(adventureworks-for-postgres-person-address_2, adventureworks-for-postgres, person, address_2, 4)]</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventureworks-for-postgres.person.address_2</td>\n",
       "      <td>[(adventureworks-for-postgres-person-address, adventureworks-for-postgres, person, address, 7)]</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adventureworks-for-postgres.person.addresstype</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adventureworks-for-postgres.person.businessentity</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adventureworks-for-postgres.person.businessentityaddress</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adventureworks-for-postgres.person.contacttype</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adventureworks-for-postgres.person.countryregion</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adventureworks-for-postgres.person.emailaddress</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adventureworks-for-postgres.person.password</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adventureworks-for-postgres.person.person</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adventureworks-for-postgres.person.personphone</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adventureworks-for-postgres.person.phonenumbertype</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adventureworks-for-postgres.person.stateprovince</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adventureworks-for-postgres.person.vadditionalcontactinfo</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.person -&gt; adventureworks-for-postgres.person.person, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo, adventureworks-for-postgres.person.vadditionalcontactinfo -&gt; adventureworks-for-postgres.person.vadditionalcontactinfo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exploration_results = data_exploration_service.explore_by_source('adventureworks-for-postgres', '0.8', '0.8', dastart, daend)\n",
    "df = format_results(exploration_results)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6ae48fd1-1579-4d20-bbbd-f58dbac48d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>related_nodes_by_signature</th>\n",
       "      <th>related_nodes_by_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adventureworks-for-postgres.person.address</td>\n",
       "      <td>[(adventureworks-for-postgres-person-address_2...</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventureworks-for-postgres.person.address_2</td>\n",
       "      <td>[(adventureworks-for-postgres-person-address, ...</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adventureworks-for-postgres.person.addresstype</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adventureworks-for-postgres.person.businessentity</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adventureworks-for-postgres.person.businessent...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adventureworks-for-postgres.person.contacttype</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adventureworks-for-postgres.person.countryregion</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adventureworks-for-postgres.person.emailaddress</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adventureworks-for-postgres.person.password</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adventureworks-for-postgres.person.person</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adventureworks-for-postgres.person.personphone</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adventureworks-for-postgres.person.phonenumber...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adventureworks-for-postgres.person.stateprovince</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.person -&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adventureworks-for-postgres.person.vadditional...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          dataset  \\\n",
       "(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)                                                NaN   \n",
       "0                                                      adventureworks-for-postgres.person.address   \n",
       "1                                                    adventureworks-for-postgres.person.address_2   \n",
       "2                                                  adventureworks-for-postgres.person.addresstype   \n",
       "3                                               adventureworks-for-postgres.person.businessentity   \n",
       "4                                               adventureworks-for-postgres.person.businessent...   \n",
       "5                                                  adventureworks-for-postgres.person.contacttype   \n",
       "6                                                adventureworks-for-postgres.person.countryregion   \n",
       "7                                                 adventureworks-for-postgres.person.emailaddress   \n",
       "8                                                     adventureworks-for-postgres.person.password   \n",
       "9                                                       adventureworks-for-postgres.person.person   \n",
       "10                                                 adventureworks-for-postgres.person.personphone   \n",
       "11                                              adventureworks-for-postgres.person.phonenumber...   \n",
       "12                                               adventureworks-for-postgres.person.stateprovince   \n",
       "13                                              adventureworks-for-postgres.person.vadditional...   \n",
       "\n",
       "                                                                       related_nodes_by_signature  \\\n",
       "(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)                                                NaN   \n",
       "0                                               [(adventureworks-for-postgres-person-address_2...   \n",
       "1                                               [(adventureworks-for-postgres-person-address, ...   \n",
       "2                                                                                              []   \n",
       "3                                                                                              []   \n",
       "4                                                                                              []   \n",
       "5                                                                                              []   \n",
       "6                                                                                              []   \n",
       "7                                                                                              []   \n",
       "8                                                                                              []   \n",
       "9                                                                                              []   \n",
       "10                                                                                             []   \n",
       "11                                                                                             []   \n",
       "12                                                                                             []   \n",
       "13                                                                                             []   \n",
       "\n",
       "                                                                         related_nodes_by_content  \n",
       "(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)                                                NaN  \n",
       "0                                               [adventureworks-for-postgres.person.vadditiona...  \n",
       "1                                               [adventureworks-for-postgres.person.vadditiona...  \n",
       "2                                               [adventureworks-for-postgres.person.person -> ...  \n",
       "3                                               [adventureworks-for-postgres.person.person -> ...  \n",
       "4                                               [adventureworks-for-postgres.person.person -> ...  \n",
       "5                                                                                              []  \n",
       "6                                                                                              []  \n",
       "7                                               [adventureworks-for-postgres.person.vadditiona...  \n",
       "8                                               [adventureworks-for-postgres.person.person -> ...  \n",
       "9                                               [adventureworks-for-postgres.person.person -> ...  \n",
       "10                                                                                             []  \n",
       "11                                                                                             []  \n",
       "12                                              [adventureworks-for-postgres.person.person -> ...  \n",
       "13                                              [adventureworks-for-postgres.person.vadditiona...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploration_results = data_exploration_service.explore_by_schema('person', '0.8', '0.8', dastart, daend)\n",
    "df = format_results(exploration_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d9dbe496-575a-409c-8c90-6c8e3cba6eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>related_nodes_by_signature</th>\n",
       "      <th>related_nodes_by_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adventureworks-for-postgres.person.vadditional...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[adventureworks-for-postgres.person.vadditiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               dataset  \\\n",
       "(0)                                                NaN   \n",
       "0    adventureworks-for-postgres.person.vadditional...   \n",
       "\n",
       "    related_nodes_by_signature  \\\n",
       "(0)                        NaN   \n",
       "0                           []   \n",
       "\n",
       "                              related_nodes_by_content  \n",
       "(0)                                                NaN  \n",
       "0    [adventureworks-for-postgres.person.vadditiona...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploration_results = data_exploration_service.explore_by_dataset('vadditionalcontactinfo', '0.8', '0.8', dastart, daend)\n",
    "df = format_results(exploration_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9f9b3-4c7c-402d-bf78-ab46e721e327",
   "metadata": {},
   "source": [
    "## Update description and add notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "42a737b2-3f34-434b-aaf9-02f0c850bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Present Notes: '\n"
     ]
    }
   ],
   "source": [
    "data_exploration_service.insert_note('adventureworks-for-postgres-person-password', 'note abc')\n",
    "res = data_exploration_service.explore_by_signature('adventureworks-for-postgres-person-password')\n",
    "\n",
    "pprint(\"Present Notes: \"%res.get().notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d1fde7fb-c2ef-430a-8b00-42470cc17d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New descriprion value: \"description updated\"\n"
     ]
    }
   ],
   "source": [
    "data_exploration_service.update_description('adventureworks-for-postgres-person-password', 'description updated')\n",
    "res = data_exploration_service.explore_by_signature('adventureworks-for-postgres-person-password')\n",
    "\n",
    "print('New descriprion value: \"%s\"'%res.get().description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b02aa7-9383-4ba8-90a6-236243c8877e",
   "metadata": {},
   "source": [
    "## Create Jobs\n",
    "\n",
    "\n",
    "\n",
    "Basierend auf der Pertinenz und QualitÃ¤t der Ergebnisse\n",
    "der Abfrage kann der Benutzer drei Aufgaben durchfÃ¼hren: Ãnderung oder\n",
    "Anreicherung von Metadaten, Definition von Dataset-Generatoren und Erstellung\n",
    "von Output-Quellen. Das Ãndern und Anreichern von Metadaten\n",
    "geben dem Benutzer die MÃ¶glichkeit, die Informationen Ã¼ber die Datasets,\n",
    "aus denen der Data Lake besteht, zu integrieren oder zu korrigieren sowie\n",
    "die Beziehungen zwischen den Datasets, die Havel automatisch identifiziert\n",
    "hat, zu Ã¼berprÃ¼fen.\n",
    "Der Dataset-Generator ist ein Job, der vom Benutzer erstellt und konfiguriert\n",
    "und vom System gemÃ¤Ã der Planungskonfiguration ausgefÃ¼hrt wird.\n",
    "Der Hauptzweck des Jobs besteht darin, die konfigurierte Abfrage auszufÃ¼hren\n",
    "und die Ergebnisse in einem neuen Datensatz mit dem Label generiert\n",
    "abzulegen. Der generierten Datensatz besitzt auÃerdem die Relation\n",
    "IS_GENERATED_FROM mit den ausgehenden Datasets, die in die Abfrage einbezogen\n",
    "wurden, und teilt mit den bestehenden dieselbe Datenstruktur. Der\n",
    "Data Lake kann auf diese Weise erweitert werden. Dataset-Generatoren dÃ¼rfen\n",
    "miteinander kombiniert werden und eine Dataset-Generatoren-Pipeline\n",
    "bilden. Nach jeder AusfÃ¼hrung wird ein Report erstellt. Die Output-Quelle\n",
    "ist ebenfalls ein Job, so wie der Dataset-Generator. Der Unterschied zwischen\n",
    "den beiden liegt darin, dass die Output-Quelle das Ergebnis ihrer Abfrage\n",
    "in einem Format speichert, das fÃ¼r den Export in andere Systeme wie z.B.\n",
    "Data Warehouse geeignet ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f79c08a7-af63-447b-9a5a-234270e93e15",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "()",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"python cell\", line 63, in <module>",
      "  File \"python cell\", line 22, in exec_job",
      "  File \"python cell\", line 11, in _exec_output_job",
      "  File \"/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py\", line 1034, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self)",
      "  File \"/usr/local/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(",
      "  File \"/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 196, in deco\n    raise converted from None",
      "AnalysisException: Table or view not found: personphone; line 1 pos 28;\n'Project [*]\n+- 'Join Inner, ('a.emailpromotion = 'b.phonenumbertypeid)\n   :- SubqueryAlias a\n   :  +- SubqueryAlias person\n   :     +- View (`person`, [businessentityid#5267,persontype#5268,namestyle#5269,title#5270,firstname#5271,middlename#5272,lastname#5273,suffix#5274,emailpromotion#5275,additionalcontactinfo#5276,demographics#5277,rowguid#5278,modifieddate#5279])\n   :        +- Relation [businessentityid#5267,persontype#5268,namestyle#5269,title#5270,firstname#5271,middlename#5272,lastname#5273,suffix#5274,emailpromotion#5275,additionalcontactinfo#5276,demographics#5277,rowguid#5278,modifieddate#5279] csv\n   +- 'SubqueryAlias b\n      +- 'UnresolvedRelation [personphone], [], false\n"
     ]
    }
   ],
   "source": [
    "class JobService:\n",
    "\n",
    "    jobs = {}\n",
    "    OUTPUT_JOB = 'output'\n",
    "\n",
    "    def exec_all_jobs(self):\n",
    "        for k in JobService.jobs.keys(): \n",
    "            self.exec_job(k).show()\n",
    "\n",
    "    def _exec_output_job(self, job_id):\n",
    "        res_df = spark.sql(JobService.jobs[job_id]['cmd'])\n",
    "        res_df.createOrReplaceTempView(JobService.jobs[job_id]['table_name'])\n",
    "\n",
    "        if (not JobService.jobs[job_id]['temp']):\n",
    "            res_df.write.mode(\"append\").insertInto(JobService.jobs[job_id]['table_name'])\n",
    "            #res_df.write.mode('overwrite').saveAsTable(JobService.jobs[job_id]['table_name'])    \n",
    "\n",
    "        return res_df\n",
    "    \n",
    "    def exec_job(self, job_id):\n",
    "        if JobService.jobs[job_id]['type'] == JobService.OUTPUT_JOB:\n",
    "            return self._exec_output_job(job_id)\n",
    "        else:\n",
    "            print('job type unknown')\n",
    "\n",
    "    def create_output_job(self, id, name, cmd, table_name, temp):\n",
    "        JobService.jobs[id] = {\n",
    "            'type': JobService.OUTPUT_JOB,\n",
    "            'name': name,\n",
    "            'cmd': cmd,\n",
    "            'table_name': table_name,\n",
    "            'temp': temp\n",
    "        }\n",
    "    \n",
    "    def create_data_config(self, schema, dataset, attributes_blacklist, keywords_whitelist):\n",
    "        return {\n",
    "            'attributes_blacklist': attributes_blacklist,\n",
    "            'dataframe': df,\n",
    "            'dataset': dataset,\n",
    "            'dataset_description': 'AUTO-GENERATED',\n",
    "            'hdfs_path': '//HAVEL/adventureworks-for-postgres/person/address__2022-11-20.csv',\n",
    "            'hdfs_uri_base': '/HAVEL/adventureworks-for-postgres/person',\n",
    "            'keywords_whitelist': keywords_whitelist,\n",
    "            'schema': schema,\n",
    "            'signature': Utils.calc_idx_resource(source, schema, dataset),\n",
    "            'source': 'HAVEL',\n",
    "            'source_description': 'AUTO-GENERATED',\n",
    "            'url': '/HAVEL',\n",
    "            'version': 1\n",
    "        }\n",
    "\n",
    "\n",
    "job_service = JobService()\n",
    "        \n",
    "# job_service.create_output_job(id='my_first_job', name='hello there', cmd='show tables', table_name='my_first_table', temp=False)\n",
    "\n",
    "job_service.create_output_job(id='my_first_joba', \n",
    "      name='hello there', \n",
    "      cmd='SELECT * FROM person a JOIN personphone b ON (a.emailpromotion=b.phonenumbertypeid)', \n",
    "      table_name='table_name_aaa', \n",
    "      temp=True)\n",
    "\n",
    "job_service.exec_job('my_first_joba').show()\n",
    "\n",
    "job_service.exec_all_jobs()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be19c52b-d3c7-4e0e-8395-9d86ec751892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|  default|       student|      false|\n",
      "|  default|       teacher|      false|\n",
      "|         |  emailaddress|       true|\n",
      "|         |my_first_table|       true|\n",
      "|         |        person|       true|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e9bc884-1c3d-4e13-8be1-d94bf7f5f762",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "()",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"python cell\", line 1, in <module>",
      "  File \"/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py\", line 1034, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self)",
      "  File \"/usr/local/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(",
      "  File \"/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 196, in deco\n    raise converted from None",
      "AnalysisException: Table or view not found: personphone; line 1 pos 28;\n'Project [*]\n+- 'Join Inner, ('a.emailpromotion = 'b.phonenumbertypeid)\n   :- SubqueryAlias a\n   :  +- SubqueryAlias person\n   :     +- View (`person`, [businessentityid#5267,persontype#5268,namestyle#5269,title#5270,firstname#5271,middlename#5272,lastname#5273,suffix#5274,emailpromotion#5275,additionalcontactinfo#5276,demographics#5277,rowguid#5278,modifieddate#5279])\n   :        +- Relation [businessentityid#5267,persontype#5268,namestyle#5269,title#5270,firstname#5271,middlename#5272,lastname#5273,suffix#5274,emailpromotion#5275,additionalcontactinfo#5276,demographics#5277,rowguid#5278,modifieddate#5279] csv\n   +- 'SubqueryAlias b\n      +- 'UnresolvedRelation [personphone], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM person a JOIN emailaddress b ON (a.businessentityid = b.emailaddressid)').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9160d6-d533-45e5-8ca6-0b4ba273ab41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (python)",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
